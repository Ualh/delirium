{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eicu_feature(min_time,skip_time):\n",
    "    \n",
    "    from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import os\n",
    "    import csv\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from scipy import interp\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from itertools import cycle\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix,f1_score,auc,roc_curve,precision_recall_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc,confusion_matrix, average_precision_score, matthews_corrcoef\n",
    "    from numpy.random import seed\n",
    "    from inspect import signature\n",
    "\n",
    "    import keras\n",
    "    from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from numpy.random import seed\n",
    "\n",
    "\n",
    "    import torch.nn as nn\n",
    "    import optuna\n",
    "    from torch.utils.data.sampler import WeightedRandomSampler\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader,Dataset\n",
    "    import torch \n",
    "    import torch.nn as nn\n",
    "    from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "    SEED_VALUE = 36\n",
    "    seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    random.seed(SEED_VALUE)\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED_VALUE)\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    \n",
    "    CV = True\n",
    "    BATCH_SIZE = 128\n",
    "    TASK = '%d_%d'%(min_time,skip_time)\n",
    "    \n",
    "    class BiRNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes, drp):\n",
    "            super(BiRNN, self).__init__()\n",
    "            self.num_layers  = num_layers\n",
    "            self.num_classes = num_classes\n",
    "            self.drop = nn.Dropout(p = drp)\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
    "            self.fc = nn.Linear(hidden_size*2, num_classes) # 2 for bidirection\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.embeds = nn.Embedding(50,5)\n",
    "            self.hidden_size= hidden_size\n",
    "\n",
    "\n",
    "        def forward(self, x_num, x_cat , seq_lengths):\n",
    "            embed = self.embeds(x_cat).reshape((x_cat.size(0), x_cat.size(1), -1))\n",
    "            x = torch.cat((x_num,embed),-1)\n",
    "            out, _ =  self.lstm(x) # tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "            lstm_out = out[:,-1,:]\n",
    "            output = self.fc(lstm_out)#torch.cat((lstm_out_fw, lstm_out_bw), -1))\n",
    "            return output\n",
    "    \n",
    "###########################################################\n",
    "    def plot_cum_auc(x_y, models, savename='plot.png', x_label='False Positive Rate', y_label='True Positive Rate',auprc=False, thr=0):\n",
    "    \n",
    "        colors = ['b', 'r','g']\n",
    "        for i, (x_y_tuple, m) in enumerate(zip(x_y, models)):\n",
    "            plt.plot(x_y_tuple[0], x_y_tuple[1],label='{}: {:.4f}'.format(m,auc(x_y_tuple[0], x_y_tuple[1])), color=colors[i])\n",
    "        if auprc:\n",
    "            plt.plot([0, 1], [thr, thr], linestyle='--', lw=2, color='k',label=\"Random: {:.4f}\".format(thr), alpha=.8)\n",
    "\n",
    "        else:\n",
    "            plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',label=\"Random: {:.4f}\".format(0.5000), alpha=.8)\n",
    "\n",
    "        plt.xlabel(x_label, fontsize=15)\n",
    "        plt.ylabel(y_label, fontsize=15)\n",
    "        plt.axhline(0, color='black')\n",
    "        plt.axvline(0, color='black')\n",
    "\n",
    "        if auprc:\n",
    "            legend = plt.legend(loc=\"upper left\", prop={'size': 10}, bbox_to_anchor=(0.6, 0.95))\n",
    "        else:\n",
    "            legend = plt.legend(loc=\"lower right\", prop={'size': 10}, bbox_to_anchor=(1, 0.05))\n",
    "            \n",
    "        plt.savefig(savename,\n",
    "                     dpi=400, facecolor='white', transparent=True, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def compute_metrics(X_test,y_test,y_probs):\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "        specat90 = 1-fpr[tpr>=0.90][0]\n",
    "        intrp = interp(np.linspace(0, 1, 100), fpr, tpr)\n",
    "        intrp[0] = 0.0\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        TN,FP,FN,TP = confusion_matrix(y_test, y_probs.round()).ravel()\n",
    "        PPV = TP/(TP+FP)\n",
    "        NPV = TN/(TN+FN)\n",
    "\n",
    "\n",
    "        mcc = matthews_corrcoef(y_test, y_probs.round())\n",
    "        \n",
    "        average_precision = average_precision_score(y_test, y_probs)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "        prs = interp(np.linspace(0, 1, 100), recall[::-1], precision[::-1])\n",
    "        prs[0] = 1.0\n",
    "        prs[-1] = 0.0\n",
    "\n",
    "        auc_prc = auc(recall, precision)\n",
    "\n",
    "        return {'specat90': specat90, 'intrp': intrp,\n",
    "                'fpr': fpr,\n",
    "                'tpr': tpr, 'auc': roc_auc,\n",
    "                'ppv': PPV, 'npv': NPV,\n",
    "                'auc_prc': auc_prc,\n",
    "                'prc':precision,\n",
    "                'prs':prs,\n",
    "                'rec':recall,\n",
    "                'mcc': mcc}\n",
    "\n",
    "\n",
    "    def avg_metrics(results):\n",
    "    \n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        mean_recall = np.linspace(0, 1, 100)\n",
    "\n",
    "        mean_tpr = np.mean([results[k]['intrp'] for k in results], axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        std_tpr = np.std([results[k]['intrp'] for k in results], axis=0)\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std([results[k]['auc'] for k in results])\n",
    "        ppvs = np.mean([results[k]['ppv'] for k in results])\n",
    "        npvs = np.mean([results[k]['npv'] for k in results])\n",
    "        mccs = np.mean([results[k]['mcc'] for k in results])\n",
    "        specat90 = np.mean([results[k]['specat90'] for k in results])\n",
    "\n",
    "        mean_precision = np.mean([results[k]['prs'] for k in results], axis=0)\n",
    "        mean_precision[-1] = 0.0\n",
    "        #########################\n",
    "        mean_precision[0] = 1.0\n",
    "        ########################\n",
    "        mean_auc_prc = auc(mean_recall, mean_precision)\n",
    "        std_auc_prc = np.std([results[k]['auc_prc'] for k in results], axis=0)\n",
    "\n",
    "        print(\"Mean AUC: {0:0.4f} +- STD{1:0.4f}\".format(mean_auc,std_auc))\n",
    "        print(\"PPV: {0:0.4f}\".format(ppvs))\n",
    "        print(\"NPV: {0:0.4f}\".format(npvs))\n",
    "        print(\"MCC: {0:0.4f}\".format(mccs))\n",
    "        print(\"Spec@90: {0:0.4f}\".format(specat90))\n",
    "\n",
    "        return {'mean_auc': mean_auc,\n",
    "                'tpr': mean_tpr,\n",
    "                'std_auc':std_auc,\n",
    "                'std_tpr': std_tpr,\n",
    "                'ppv':ppvs,\n",
    "                'npv':npvs,\n",
    "                'mean_auc_prc':mean_auc_prc,\n",
    "                'std_auc_prc':std_auc_prc,\n",
    "                'mean_prc':mean_precision,\n",
    "                'mean_recall':mean_recall,\n",
    "                'mcc':mccs,\n",
    "                'spec@90':specat90}\n",
    "\n",
    "    def plot_auc(result):\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red',\n",
    "                 label='Random', alpha=.8)\n",
    "\n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        mean_tpr = result['tpr']\n",
    "        mean_auc = result['mean_auc']\n",
    "        std_auc = result['std_auc']\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.4f $\\pm$ %0.4f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_tpr = result['std_tpr']\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "###########################################################################\n",
    "    \n",
    "    #pos\n",
    "    def pos_selection(df_pos,skip_time,min_time):\n",
    "        posl = []\n",
    "        posdf = pd.DataFrame(columns=df_pos.columns)\n",
    "        all_matches = df_pos[df_pos['labelpt'] == df_pos['labelrec']]\n",
    "\n",
    "        for i, p_id in enumerate(all_matches['patientunitstayid'].unique()):\n",
    "            df_p_id = df_pos[df_pos['patientunitstayid'] == p_id]\n",
    "            idx = all_matches[all_matches['patientunitstayid'] == p_id].index[0]\n",
    "            t = df_pos.iloc[idx].itemoffset\n",
    "            if t > (min_time + skip_time):\n",
    "                posl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) & (df_p_id['itemoffset'] > (t-(skip_time+min_time)))])\n",
    "\n",
    "        posdf = pd.concat(posl,axis=0)\n",
    "        return posdf\n",
    "\n",
    "    #neg\n",
    "    def neg_selection(df_neg,skip_time,min_time):\n",
    "        negl = []\n",
    "        negdf = pd.DataFrame(columns=df_neg.columns)\n",
    "\n",
    "        for i, p_id in enumerate(df_neg['patientunitstayid'].unique()):\n",
    "            df_p_id = df_neg[df_neg['patientunitstayid'] == p_id]\n",
    "            t = df_p_id.iloc[-1].itemoffset\n",
    "            if t > (min_time + skip_time):\n",
    "                negl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) &(df_p_id['itemoffset'] > (t-(min_time+skip_time)))])\n",
    "        negdf = pd.concat(negl,axis=0)\n",
    "        return negdf\n",
    "    eicu_df  = pd.read_csv(\"eicu_df_all_24los_normed.csv\")\n",
    "\n",
    " # ADD VASO\n",
    "    eicu_df['vaso_dose'] = np.nan\n",
    "    eicu_df['vaso_dose'] = eicu_df['rate_epinephrine']+eicu_df['rate_norepinephrine'] + eicu_df['rate_phenylephrine']/10 + eicu_df['rate_dopamine']/2\n",
    "    eicu_df.drop(columns=['rate_epinephrine', 'rate_norepinephrine','rate_phenylephrine','rate_dopamine'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    eicu_pos = eicu_df[eicu_df['CAM']==1]\n",
    "    eicu_neg = eicu_df[eicu_df['CAM']==0]\n",
    "    \n",
    "    eicu_pos_filtered = pos_selection(eicu_pos,skip_time,min_time)\n",
    "    eicu_neg_filtered = neg_selection(eicu_neg,skip_time,min_time)\n",
    "    eicu_df_filtered = pd.concat([eicu_pos_filtered, eicu_neg_filtered],axis=0)\n",
    "\n",
    "\n",
    "    trg = eicu_df_filtered.groupby('patientunitstayid')\n",
    "\n",
    "\n",
    "    idtr = []\n",
    "    train_np = []\n",
    "    for idx, frame in trg:\n",
    "        idtr.append(idx)\n",
    "        train_np.append(frame)\n",
    "\n",
    "\n",
    "    columns_ord = ['patientunitstayid', 'itemoffset', 'gender','sofa', 'sofa_wo_gcs', 'age', 'admissionheight',\n",
    "           'admissionweight', 'Heart Rate', 'O2 Saturation', 'glucose',\n",
    "           'Temperature (C)', 'sodium', 'BUN', 'WBC x 1000', 'Hemoglobin',\n",
    "           'Platelets', 'Potassium', 'Chloride', 'Bicarbonate', 'Creatinine',\n",
    "            'vent_flag', 'vaso_dose', 'CAM']\n",
    "\n",
    "    def reader_deli(df_list,verbose=1):\n",
    "        X_noncat = []\n",
    "        X_cat = []\n",
    "        deli = []\n",
    "        nrows = []\n",
    "        ts = []\n",
    "        PID = []\n",
    "        nb_unit_stays = len(df_list)\n",
    "        for i, df in enumerate(df_list):\n",
    "            if verbose:\n",
    "                sys.stdout.write('\\rFeed StayID {0} of {1}...'.format(i+1, nb_unit_stays))\n",
    "            dft = df\n",
    "            dummy = pd.DataFrame(columns=columns_ord)\n",
    "            for c in columns_ord:\n",
    "                dummy[c] = dft[c]        \n",
    "            dft = dummy\n",
    "            narr = np.array(dft)\n",
    "            pid = narr[0,0]\n",
    "            x_cat    = narr[:,2:5]\n",
    "            x_noncat = narr[:, 5:-1]\n",
    "            labeldeli = narr[0, -1]\n",
    "            time = narr[:,1]\n",
    "            X_cat.append(x_cat)\n",
    "            X_noncat.append(x_noncat)\n",
    "            deli.append(labeldeli)\n",
    "            ts.append(time)\n",
    "            nrows.append(narr.shape[0])\n",
    "            PID.append(pid)\n",
    "        PID = np.array(PID)    \n",
    "        X_cat = np.array(X_cat)\n",
    "        X_noncat = np.array(X_noncat)\n",
    "        deli = np.array(deli)\n",
    "        ts= np.array(ts)\n",
    "        return PID,X_cat,X_noncat,ts,nrows,deli\n",
    "    \n",
    "    PID_tr,X_cat_tr,X_num_tr,ts_tr,nrows_tr,y_tr = reader_deli(train_np)\n",
    "    \n",
    "    def pad(arr, max_len= min_time):\n",
    "        tmp = np.zeros((max_len, arr.shape[1]))\n",
    "        tmp[:arr.shape[0], :arr.shape[1]] = arr\n",
    "        return tmp  \n",
    "    def ret_numpy(X):\n",
    "        X_list = []\n",
    "        for n in X:\n",
    "            X_list.append(pad(n))\n",
    "        return np.array(X_list)\n",
    "    \n",
    "    X_cat_tr = ret_numpy(X_cat_tr)\n",
    "    X_num_tr = ret_numpy(X_num_tr)\n",
    "\n",
    "    \n",
    "    X_train = np.concatenate([X_num_tr,X_cat_tr],axis=-1)\n",
    "    y_train = y_tr\n",
    "\n",
    "    \n",
    "    def data_reader(X,y):\n",
    "        return X,y\n",
    "\n",
    "    \n",
    "    class overdata(Dataset):\n",
    "        def __init__(self, data,label):\n",
    "            self.data  = torch.FloatTensor(data.astype('float'))\n",
    "            self.label = torch.FloatTensor(label.astype('float'))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            label  = self.label[index]\n",
    "            data   = self.data[index]\n",
    "            return data,label\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "\n",
    "    def sampler_fn(data, label):\n",
    "        train_class_sample_count = torch.tensor([(torch.tensor(label) == t).sum() for t in torch.unique(torch.tensor(label), sorted=True)])\n",
    "        train_weight = 1 / train_class_sample_count.float()\n",
    "        train_samples_weight = torch.tensor([train_weight[i] for i in torch.tensor(label).long()])\n",
    "        train_sampler = WeightedRandomSampler(train_samples_weight, len(train_samples_weight))\n",
    "        return train_sampler\n",
    "    \n",
    "    def get_class_weights(data, label):\n",
    "        neg = data[label==0].shape[0]\n",
    "        pos = len(data) - neg\n",
    "        weight_for_0 = (1 / neg)*((pos+neg))/2.0 \n",
    "        weight_for_1 = (1 / pos)*((pos+neg))/2.0\n",
    "        return weight_for_0 ,(weight_for_1)\n",
    "    \n",
    "    if CV:\n",
    "        X_train,y_train= data_reader(X_train,y_train)\n",
    "        \n",
    "    sequence_length = min_time\n",
    "    input_size = 33 \n",
    "        \n",
    "    def run_train(data_loader,params,data,label):\n",
    "    \n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        cw_0,cw_1 = get_class_weights(data,label)\n",
    "        model = BiRNN(input_size, params['hidden_units'], params['num_layers'], num_classes=params['n_classes'],drp = params['dropout']).to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.Tensor([cw_0,cw_1]).to(device))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'],weight_decay=1e-6)\n",
    "\n",
    "        for epoch in range(params['epochs']):\n",
    "\n",
    "            true_labels = []\n",
    "            pred_labels = []\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            for data, labels in data_loader:\n",
    "#                 import pdb;pdb.set_trace()\n",
    "                model.train()\n",
    "                data = torch.FloatTensor(data).to(device)\n",
    "                labels = torch.FloatTensor(labels).to(device)\n",
    "                labels = labels.long()\n",
    "\n",
    "                ##\n",
    "                nonzeros = ~data.cpu().numpy().any(axis=2)\n",
    "                nonzero_index = np.argmax(nonzeros,axis=1)        \n",
    "                nonzero_index = torch.LongTensor(nonzero_index)\n",
    "    #             import pdb;pdb.set_trace()\n",
    "                nonzero_index = torch.where(nonzero_index==0,torch.LongTensor([12]),nonzero_index)\n",
    "                seq_lengths, perm_idx = nonzero_index.sort(0, descending=True)\n",
    "\n",
    "                data = data[perm_idx]\n",
    "                labels = labels[perm_idx]\n",
    "                ###\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data[:, :, :18], data[:, : ,18:].long(), seq_lengths)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()        \n",
    "                running_loss += loss.item()\n",
    "            print(\"Epoch: {}/{} \".format(epoch+1, params['epochs']),\"Training Loss: {:.3f} \".format(running_loss/len(data_loader)))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    params = {'lr':0.000075,'num_layers':1,'hidden_units':128,'n_classes':2,\n",
    "              'dropout':0.3,'epochs':50}\n",
    "    \n",
    "    def run_test(data_loader,model):\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "            true_labels, pred_labels = [], []\n",
    "\n",
    "            for data,labels in data_loader:\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.long()\n",
    "                ###\n",
    "                nonzeros = ~data.cpu().numpy().any(axis=2)\n",
    "                data = torch.Tensor(data).to(device)\n",
    "                nonzero_index = np.argmax(nonzeros,axis=1)        \n",
    "                nonzero_index = torch.LongTensor(nonzero_index)\n",
    "                nonzero_index = torch.where(nonzero_index==0,torch.LongTensor([12]),nonzero_index)\n",
    "\n",
    "                seq_lengths, perm_idx = nonzero_index.sort(0, descending=True)\n",
    "                data = data[perm_idx]\n",
    "                labels = labels[perm_idx]\n",
    "                ###\n",
    "                outputs = nn.Softmax()(model(data[:, :, :18], data[:, : ,18:].long(), seq_lengths))\n",
    "                pred = outputs.detach().cpu().numpy()\n",
    "                pred = pred[:,1]\n",
    "                true_labels.append(labels)\n",
    "                pred_labels.append(pred)\n",
    "            true_labels = [l.item() for labels in true_labels for l in labels]\n",
    "            pred_labels = [p for labels in pred_labels for p in labels]\n",
    "            pred_labels = np.array(pred_labels)\n",
    "\n",
    "            return true_labels,pred_labels\n",
    "\n",
    "    def model_cv(X,y):\n",
    "        i = 1\n",
    "        cv_scores = {}\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED_VALUE)\n",
    "        for train, test in kfold.split(X, y):\n",
    "            print(f'Fold: {i}')\n",
    "            X_tr = X[train]\n",
    "            X_ts = X[test]\n",
    "            y_tr = y[train]\n",
    "            y_ts = y[test]\n",
    "\n",
    "            data_sampler = sampler_fn(X_tr,y_tr)\n",
    "            train_data = overdata(X_tr,y_tr)\n",
    "            test_data  = overdata(X_ts,y_ts)\n",
    "            train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,shuffle=False,sampler=data_sampler, **kwargs)\n",
    "            test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=False, **kwargs)\n",
    "\n",
    "            model = run_train(train_loader, params, X_tr, y_tr)\n",
    "            savepath = \"eicu_%d_%d_fold_%d_cv_torch.pt\"%(min_time,skip_time,i)\n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.save(model.state_dict(), savepath)\n",
    "            print(f\"Model:{savepath} saved.\")\n",
    "\n",
    "            y_ts,probs = run_test(test_loader,model)\n",
    "            cv_scores[i] = compute_metrics(X_ts,y_ts,probs)\n",
    "            i += 1        \n",
    "        cum_scores = avg_metrics(cv_scores)\n",
    "        return cum_scores\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    models = ['LSTM']\n",
    "    results = {}\n",
    "    for m in models:\n",
    "        print(f'**********Model: {m} *********')\n",
    "        results[m] = model_cv(X_train,y_train)\n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        fpr_tprs = [(mean_fpr, results[m]['tpr']) for m in models]\n",
    "        plot_cum_auc(fpr_tprs, models, savename='{}_cv_eicu_feat.png'.format(TASK),auprc=False)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [12]:\n",
    "    for s in [48]:\n",
    "        get_eicu_feature(m,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "min_time = 24\n",
    "skip_time = 96\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "\n",
    "\n",
    "#pos\n",
    "def pos_selection(df_pos,skip_time,min_time):\n",
    "    posl = []\n",
    "    posdf = pd.DataFrame(columns=df_pos.columns)\n",
    "    all_matches = df_pos[df_pos['labelpt'] == df_pos['labelrec']]\n",
    "\n",
    "    for i, p_id in enumerate(all_matches['patientunitstayid'].unique()):\n",
    "        df_p_id = df_pos[df_pos['patientunitstayid'] == p_id]\n",
    "        idx = all_matches[all_matches['patientunitstayid'] == p_id].index[0]\n",
    "        t = df_pos.iloc[idx].itemoffset\n",
    "        if t > (min_time + skip_time):\n",
    "            posl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) & (df_p_id['itemoffset'] > (t-(skip_time+min_time)))])\n",
    "\n",
    "    posdf = pd.concat(posl,axis=0)\n",
    "    return posdf\n",
    "\n",
    "#neg\n",
    "def neg_selection(df_neg,skip_time,min_time):\n",
    "    negl = []\n",
    "    negdf = pd.DataFrame(columns=df_neg.columns)\n",
    "\n",
    "    for i, p_id in enumerate(df_neg['patientunitstayid'].unique()):\n",
    "        df_p_id = df_neg[df_neg['patientunitstayid'] == p_id]\n",
    "        t = df_p_id.iloc[-1].itemoffset\n",
    "        if t > (min_time + skip_time):\n",
    "            negl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) &(df_p_id['itemoffset'] > (t-(min_time+skip_time)))])\n",
    "    negdf = pd.concat(negl,axis=0)\n",
    "    return negdf\n",
    "eicu_df  = pd.read_csv(\"eicu_df_all_24los_normed.csv\")\n",
    "# mimic_df = pd.read_csv(\"mimic_df_all_24los_normed.csv\")\n",
    "\n",
    "# ADD VASO\n",
    "eicu_df['vaso_dose'] = np.nan\n",
    "eicu_df['vaso_dose'] = eicu_df['rate_epinephrine']+eicu_df['rate_norepinephrine'] + eicu_df['rate_phenylephrine']/10 + eicu_df['rate_dopamine']/2\n",
    "eicu_df.drop(columns=['rate_epinephrine', 'rate_norepinephrine','rate_phenylephrine','rate_dopamine'],inplace=True)\n",
    "\n",
    "#     import pdb;pdb.set_trace()\n",
    "\n",
    "eicu_pos = eicu_df[eicu_df['CAM']==1]\n",
    "eicu_neg = eicu_df[eicu_df['CAM']==0]\n",
    "\n",
    "eicu_pos_filtered = pos_selection(eicu_pos,skip_time,min_time)\n",
    "eicu_neg_filtered = neg_selection(eicu_neg,skip_time,min_time)\n",
    "eicu_df_filtered = pd.concat([eicu_pos_filtered, eicu_neg_filtered],axis=0)\n",
    "\n",
    "trg = eicu_df_filtered.groupby('patientunitstayid')\n",
    "idtr = []\n",
    "train_np = []\n",
    "for idx, frame in trg:\n",
    "    idtr.append(idx)\n",
    "    train_np.append(frame)\n",
    "\n",
    "\n",
    "columns_ord = ['patientunitstayid', 'itemoffset', 'gender','sofa', 'sofa_wo_gcs', 'age', 'admissionheight',\n",
    "       'admissionweight', 'Heart Rate', 'O2 Saturation', 'glucose',\n",
    "       'Temperature (C)', 'sodium', 'BUN', 'WBC x 1000', 'Hemoglobin',\n",
    "       'Platelets', 'Potassium', 'Chloride', 'Bicarbonate', 'Creatinine',\n",
    "        'vent_flag', 'vaso_dose', 'CAM']\n",
    "\n",
    "def reader_deli(df_list,verbose=1):\n",
    "    X_noncat = []\n",
    "    X_cat = []\n",
    "    deli = []\n",
    "    nrows = []\n",
    "    ts = []\n",
    "    PID = []\n",
    "    nb_unit_stays = len(df_list)\n",
    "    for i, df in enumerate(df_list):\n",
    "        if verbose:\n",
    "            sys.stdout.write('\\rFeed StayID {0} of {1}...'.format(i+1, nb_unit_stays))\n",
    "        dft = df\n",
    "        dummy = pd.DataFrame(columns=columns_ord)\n",
    "        for c in columns_ord:\n",
    "            dummy[c] = dft[c]        \n",
    "        dft = dummy\n",
    "        narr = np.array(dft)\n",
    "        pid = narr[0,0]\n",
    "        x_cat    = narr[:,2:5]\n",
    "        x_noncat = narr[:, 5:-1]\n",
    "        labeldeli = narr[0, -1]\n",
    "        time = narr[:,1]\n",
    "        X_cat.append(x_cat)\n",
    "        X_noncat.append(x_noncat)\n",
    "        deli.append(labeldeli)\n",
    "        ts.append(time)\n",
    "        nrows.append(narr.shape[0])\n",
    "        PID.append(pid)\n",
    "    PID = np.array(PID)    \n",
    "    X_cat = np.array(X_cat)\n",
    "    X_noncat = np.array(X_noncat)\n",
    "    deli = np.array(deli)\n",
    "    ts= np.array(ts)\n",
    "    return PID,X_cat,X_noncat,ts,nrows,deli\n",
    "\n",
    "PID_tr,X_cat_tr,X_num_tr,ts_tr,nrows_tr,y_tr = reader_deli(train_np)\n",
    "\n",
    "def pad(arr, max_len= min_time):\n",
    "    tmp = np.zeros((max_len, arr.shape[1]))\n",
    "    tmp[:arr.shape[0], :arr.shape[1]] = arr\n",
    "    return tmp  \n",
    "def ret_numpy(X):\n",
    "    X_list = []\n",
    "    for n in X:\n",
    "        X_list.append(pad(n))\n",
    "    return np.array(X_list)\n",
    "\n",
    "X_cat_tr = ret_numpy(X_cat_tr)\n",
    "X_num_tr = ret_numpy(X_num_tr)\n",
    "\n",
    "X_train = np.concatenate([X_num_tr,X_cat_tr],axis=-1)\n",
    "y_train = y_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score,auc,roc_curve,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix, average_precision_score, matthews_corrcoef\n",
    "from numpy.random import seed\n",
    "from inspect import signature\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import seed\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "\n",
    "from captum.attr import IntegratedGradients,NoiseTunnel\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "from captum.attr import TokenReferenceBase,remove_interpretable_embedding_layer, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "from scipy import stats\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "SEED_VALUE = 36\n",
    "seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED_VALUE)\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = ['Age', 'Height', 'Weight', 'Heart Rate', 'O2 Saturation', 'glucose',\n",
    "       'Temperature', 'Sodium', 'BUN', 'WBC', 'Hemoglobin',\n",
    "       'Platelets', 'Potassium', 'Chloride', 'Bicarbonate', 'Creatinine',\n",
    "        'Ventilation', 'Vasopressor dose', 'Gender','Sofa', 'Sofa_w/o_gcs']\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "def get_baseline(X,y):\n",
    "    baseline_num = torch.zeros(X.shape[0],min_time,18)\n",
    "    baseline_cat = torch.zeros(X.shape[0],min_time,3)\n",
    "    m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    baseline_num_gaus = m.sample((X.shape[0],min_time,18)).squeeze()  \n",
    "    baseline_num_unif = torch.FloatTensor(X.shape[0],min_time,18).uniform_(0, 1)\n",
    "    baseline_num = (baseline_num_gaus + baseline_num_unif)/2\n",
    "    x_baseline = torch.cat((baseline_num,baseline_cat),-1)\n",
    "    y_baseline = torch.ones(X.shape[0])\n",
    "    x_baseline = x_baseline.cpu().numpy()\n",
    "    y_baseline = y_baseline.cpu().numpy()\n",
    "    \n",
    "    return x_baseline, y_baseline\n",
    "\n",
    "class overdata(Dataset):\n",
    "    def __init__(self, data,label):\n",
    "        self.data  = torch.FloatTensor(data.astype('float'))\n",
    "        self.label = torch.FloatTensor(label.astype('float'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label  = self.label[index]\n",
    "        data   = self.data[index]\n",
    "        return data,label\n",
    "    \n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, drp):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.num_layers  = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.drop = nn.Dropout(p = drp)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes) # 2 for bidirection\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.embeds = nn.Embedding(50,5)\n",
    "        self.hidden_size= hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, x_num, x_cat , seq_lengths):\n",
    "        embed = self.embeds(x_cat).reshape((x_cat.size(0), x_cat.size(1), -1))\n",
    "        x = torch.cat((x_num,embed),-1)\n",
    "        out, _ =  self.lstm(x) # tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        lstm_out = out[:,-1,:]\n",
    "        output = self.fc(lstm_out)#torch.cat((lstm_out_fw, lstm_out_bw), -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap import summary_plot\n",
    "from captum.attr import ShapleyValueSampling,IntegratedGradients,GuidedBackprop\n",
    "\n",
    "alg = [\"GB\",\"IG\",\"SVS\"]\n",
    "\n",
    "\n",
    "def get_feature_ranking_for_shap(model,test_loader,base_loader,alg):\n",
    "    \n",
    "    interpretable_embedding = configure_interpretable_embedding_layer(model, \"embeds\")\n",
    "\n",
    "    total_attr_cat, total_attr_num, X_ts_mask, data_ = [], [], [], []\n",
    "    test_labels = []\n",
    "\n",
    "    model.train()\n",
    "    if alg == \"GB\":\n",
    "        gb = GuidedBackprop(model)  \n",
    "    elif alg == \"IG\":\n",
    "        ig = IntegratedGradients(model)\n",
    "    elif alg == \"SVS\":\n",
    "        svs = ShapleyValueSampling(model)\n",
    "        \n",
    "    for (data,labels),(data_base,labels_base) in zip(test_loader,base_loader):\n",
    "        nonzeros = ~data.cpu().numpy().any(axis=2)\n",
    "        data = torch.Tensor(data).to(device)\n",
    "        data_base = torch.Tensor(data_base).to(device)\n",
    "\n",
    "        nonzero_index = np.argmax(nonzeros,axis=1)        \n",
    "        nonzero_index = torch.LongTensor(nonzero_index)\n",
    "        nonzero_index = torch.where(nonzero_index==0,torch.LongTensor([min_time]),nonzero_index)\n",
    "        seq_lengths, perm_idx = nonzero_index.sort(0, descending=True)\n",
    "\n",
    "        data = data[perm_idx]\n",
    "        labels = labels[perm_idx]\n",
    "\n",
    "        data_base = data_base[perm_idx]\n",
    "        labels_base = labels_base[perm_idx]\n",
    "      \n",
    "        X_ts_bool = np.all(data.cpu().numpy()==0, axis = 2)\n",
    "        X_ts_mask.append(X_ts_bool) \n",
    "\n",
    "        categorical = interpretable_embedding.indices_to_embeddings(data[:, :, 18:].long())\n",
    "        numerical   = data[:, : ,:18]\n",
    "\n",
    "        cat_base = interpretable_embedding.indices_to_embeddings(data_base[:, :, 18:].long())\n",
    "        num_base   = data_base[:, : ,:18]\n",
    "\n",
    "        if alg == \"GB\":\n",
    "            (attr_num,attr_cat) = gb.attribute(inputs=(numerical,categorical), additional_forward_args= seq_lengths,target=1)\n",
    "        elif alg == \"IG\":\n",
    "            (attr_num,attr_cat) = ig.attribute(inputs=(numerical,categorical),baselines=(num_base,cat_base), additional_forward_args= seq_lengths,target=1)\n",
    "        elif alg == \"SVS\":\n",
    "            (attr_num,attr_cat) = svs.attribute(inputs=(numerical,categorical),baselines=(num_base,cat_base), additional_forward_args= seq_lengths,target=1)\n",
    "\n",
    "    \n",
    "        total_attr_cat.append(attr_cat.detach().cpu().numpy())\n",
    "        total_attr_num.append(attr_num.detach().cpu().numpy())\n",
    "        \n",
    "        data_.append(data.detach().cpu().numpy())\n",
    "    \n",
    "    \n",
    "\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
    "\n",
    "    total_attr_cat = np.vstack(total_attr_cat)\n",
    "    total_attr_cat = np.mean(total_attr_cat,axis=-1)\n",
    "\n",
    "    total_attr_num = np.vstack(total_attr_num)\n",
    "\n",
    "    X_ts_mask = np.vstack(X_ts_mask)\n",
    "\n",
    "    total_attr_1 = np.concatenate([total_attr_num,total_attr_cat],axis=-1)\n",
    "    total_attr_1[X_ts_mask] = np.nan\n",
    "\n",
    "    feature_imp_1 = np.nanmean(total_attr_1, axis=1) #p \n",
    "    \n",
    "    data_for_shap = np.vstack(data_)\n",
    "    \n",
    "    return feature_imp_1,data_for_shap\n",
    "\n",
    "\n",
    "def get_ranking_cv_for_shap(X,y,alg=\"GB\"):\n",
    "    i = 1\n",
    "    ranking = []\n",
    "    data_= []\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED_VALUE)\n",
    "    for train, test in kfold.split(X, y):\n",
    "        print(f'Fold: {i}')\n",
    "        X_tr = X[train]\n",
    "        X_ts = X[test]\n",
    "        y_tr = y[train]\n",
    "        y_ts = y[test]\n",
    "\n",
    "        X_base,y_base = get_baseline(X_ts,y_ts)\n",
    "        ########################################################################\n",
    "        params = {'lr':0.000075,'num_layers':1,'hidden_units':128,'n_classes':2,\n",
    "              'dropout':0.3,'epochs':50,'input_size':33}\n",
    "\n",
    "        LOADPATH = \"eicu_%d_%d_fold_%d_cv_torch.pt\"%(min_time,skip_time,i)\n",
    "\n",
    "        BATCH_SIZE = 128\n",
    "        ########################################################################\n",
    "        print(LOADPATH)\n",
    "        \n",
    "        base_data    = overdata(X_base,y_base)\n",
    "        test_data    = overdata(X_ts,y_ts)\n",
    "        \n",
    "        \n",
    "        base_loader  = DataLoader(base_data, batch_size=BATCH_SIZE,shuffle=False,**kwargs)\n",
    "        test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=False,**kwargs)\n",
    "\n",
    "        model = BiRNN(params['input_size'], params['hidden_units'], params['num_layers'], num_classes=params['n_classes'],drp = params['dropout']).to(device)\n",
    "        model.load_state_dict(torch.load(LOADPATH))\n",
    "        i += 1  \n",
    "        \n",
    "        if alg == \"GB\":\n",
    "            feat_rank,data_for_shap = get_feature_ranking_for_shap(model,test_loader,base_loader,alg=\"GB\")\n",
    "        elif alg == \"IG\":\n",
    "            feat_rank,data_for_shap = get_feature_ranking_for_shap(model,test_loader,base_loader,alg=\"IG\")\n",
    "        elif alg == \"SVS\":\n",
    "            feat_rank,data_for_shap = get_feature_ranking_for_shap(model,test_loader,base_loader,alg=\"SVS\")\n",
    "            \n",
    "        data_.append(data_for_shap)\n",
    "        ranking.append(feat_rank)\n",
    "    \n",
    "    return ranking, data_\n",
    "\n",
    "ranking_per_fold,data_per_fold = get_ranking_cv_for_shap(X_train,y_train,alg=\"GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 40}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importances_dot(figname,feature_names, importances, alg, plot=True, axis_title=\"Features\"):\n",
    "    summary_plot(importances,features=np.nanmean(np.vstack(data_per_fold),axis=1) ,feature_names=feature_name, show=False,plot_type='dot', sort=True,max_display=21, use_log_scale=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if alg == \"GB\":\n",
    "        plt.xlabel('Gradient Values')\n",
    "    elif alg == \"IG\":\n",
    "        plt.xlabel('Integrated Gradient')\n",
    "    elif alg == \"SVS\":\n",
    "        plt.xlabel('SHAP Values')\n",
    "\n",
    "    plt.savefig(figname, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if alg == \"GB\":\n",
    "    savefigname = \"{}min_{}skip_eicu_{}_jama\".format(min_time,skip_time,alg)\n",
    "    visualize_importances_dot(savefigname,feature_name, np.vstack(ranking_per_fold),alg=\"GB\")    \n",
    "elif alg == \"IG\":\n",
    "    savefigname = \"{}min_{}skip_eicu_{}_jama\".format(min_time,skip_time,alg)\n",
    "    visualize_importances_dot(savefigname,feature_name, np.vstack(ranking_per_fold),alg=\"IG\")\n",
    "elif alg == \"SVS\":\n",
    "    savefigname = \"{}min_{}skip_eicu_{}_jama\".format(min_time,skip_time,alg)\n",
    "    visualize_importances_dot(savefigname,feature_name, np.vstack(ranking_per_fold),alg=\"SVS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mimic_feature(min_time,skip_time):\n",
    "    \n",
    "    from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import os\n",
    "    import csv\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import random\n",
    "    from scipy import interp\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from itertools import cycle\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix,f1_score,auc,roc_curve,precision_recall_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc,confusion_matrix, average_precision_score, matthews_corrcoef\n",
    "    from numpy.random import seed\n",
    "    from inspect import signature\n",
    "\n",
    "    import keras\n",
    "    from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from numpy.random import seed\n",
    "\n",
    "\n",
    "    import torch.nn as nn\n",
    "    import optuna\n",
    "    from torch.utils.data.sampler import WeightedRandomSampler\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader,Dataset\n",
    "    import torch \n",
    "    import torch.nn as nn\n",
    "    from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "    SEED_VALUE = 36\n",
    "    seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    random.seed(SEED_VALUE)\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED_VALUE)\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    \n",
    "    CV = True\n",
    "    BATCH_SIZE = 128\n",
    "    TASK = '%d_%d'%(min_time,skip_time)\n",
    "    \n",
    "    class BiRNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes, drp):\n",
    "            super(BiRNN, self).__init__()\n",
    "            self.num_layers  = num_layers\n",
    "            self.num_classes = num_classes\n",
    "            self.drop = nn.Dropout(p = drp)\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
    "            self.fc = nn.Linear(hidden_size*2, num_classes) # 2 for bidirection\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.embeds = nn.Embedding(50,5)\n",
    "            self.hidden_size= hidden_size\n",
    "\n",
    "\n",
    "        def forward(self, x_num, x_cat , seq_lengths):\n",
    "            embed = self.embeds(x_cat).reshape((x_cat.size(0), x_cat.size(1), -1))\n",
    "            x = torch.cat((x_num,embed),-1)\n",
    "            out, _ =  self.lstm(x) # tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "            lstm_out = out[:,-1,:]\n",
    "            output = self.fc(lstm_out)#torch.cat((lstm_out_fw, lstm_out_bw), -1))\n",
    "            return output\n",
    "    \n",
    "###########################################################\n",
    "    def plot_cum_auc(x_y, models, savename='plot.png', x_label='False Positive Rate', y_label='True Positive Rate',auprc=False, thr=0):\n",
    "    \n",
    "        colors = ['b', 'r','g']\n",
    "        for i, (x_y_tuple, m) in enumerate(zip(x_y, models)):\n",
    "            plt.plot(x_y_tuple[0], x_y_tuple[1],label='{}: {:.4f}'.format(m,auc(x_y_tuple[0], x_y_tuple[1])), color=colors[i])\n",
    "        if auprc:\n",
    "            plt.plot([0, 1], [thr, thr], linestyle='--', lw=2, color='k',label=\"Random: {:.4f}\".format(thr), alpha=.8)\n",
    "\n",
    "        else:\n",
    "            plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',label=\"Random: {:.4f}\".format(0.5000), alpha=.8)\n",
    "\n",
    "        plt.xlabel(x_label, fontsize=15)\n",
    "        plt.ylabel(y_label, fontsize=15)\n",
    "        plt.axhline(0, color='black')\n",
    "        plt.axvline(0, color='black')\n",
    "\n",
    "        if auprc:\n",
    "            legend = plt.legend(loc=\"upper left\", prop={'size': 10}, bbox_to_anchor=(0.6, 0.95))\n",
    "        else:\n",
    "            legend = plt.legend(loc=\"lower right\", prop={'size': 10}, bbox_to_anchor=(1, 0.05))\n",
    "            \n",
    "        plt.savefig(savename,\n",
    "                     dpi=400, facecolor='white', transparent=True, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def compute_metrics(X_test,y_test,y_probs):\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "        specat90 = 1-fpr[tpr>=0.90][0]\n",
    "        intrp = interp(np.linspace(0, 1, 100), fpr, tpr)\n",
    "        intrp[0] = 0.0\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        TN,FP,FN,TP = confusion_matrix(y_test, y_probs.round()).ravel()\n",
    "        PPV = TP/(TP+FP)\n",
    "        NPV = TN/(TN+FN)\n",
    "\n",
    "\n",
    "        mcc = matthews_corrcoef(y_test, y_probs.round())\n",
    "        \n",
    "        average_precision = average_precision_score(y_test, y_probs)\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "        prs = interp(np.linspace(0, 1, 100), recall[::-1], precision[::-1])\n",
    "        prs[0] = 1.0\n",
    "        prs[-1] = 0.0\n",
    "\n",
    "        auc_prc = auc(recall, precision)\n",
    "\n",
    "        return {'specat90': specat90, 'intrp': intrp,\n",
    "                'fpr': fpr,\n",
    "                'tpr': tpr, 'auc': roc_auc,\n",
    "                'ppv': PPV, 'npv': NPV,\n",
    "                'auc_prc': auc_prc,\n",
    "                'prc':precision,\n",
    "                'prs':prs,\n",
    "                'rec':recall,\n",
    "                'mcc': mcc}\n",
    "\n",
    "\n",
    "    def avg_metrics(results):\n",
    "    \n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        mean_recall = np.linspace(0, 1, 100)\n",
    "\n",
    "        mean_tpr = np.mean([results[k]['intrp'] for k in results], axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        std_tpr = np.std([results[k]['intrp'] for k in results], axis=0)\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std([results[k]['auc'] for k in results])\n",
    "        ppvs = np.mean([results[k]['ppv'] for k in results])\n",
    "        npvs = np.mean([results[k]['npv'] for k in results])\n",
    "        mccs = np.mean([results[k]['mcc'] for k in results])\n",
    "        specat90 = np.mean([results[k]['specat90'] for k in results])\n",
    "\n",
    "        mean_precision = np.mean([results[k]['prs'] for k in results], axis=0)\n",
    "        mean_precision[-1] = 0.0\n",
    "        #########################\n",
    "        mean_precision[0] = 1.0\n",
    "        ########################\n",
    "        mean_auc_prc = auc(mean_recall, mean_precision)\n",
    "        std_auc_prc = np.std([results[k]['auc_prc'] for k in results], axis=0)\n",
    "\n",
    "        print(\"Mean AUC: {0:0.4f} +- STD{1:0.4f}\".format(mean_auc,std_auc))\n",
    "        print(\"PPV: {0:0.4f}\".format(ppvs))\n",
    "        print(\"NPV: {0:0.4f}\".format(npvs))\n",
    "        print(\"MCC: {0:0.4f}\".format(mccs))\n",
    "        print(\"Spec@90: {0:0.4f}\".format(specat90))\n",
    "\n",
    "        return {'mean_auc': mean_auc,\n",
    "                'tpr': mean_tpr,\n",
    "                'std_auc':std_auc,\n",
    "                'std_tpr': std_tpr,\n",
    "                'ppv':ppvs,\n",
    "                'npv':npvs,\n",
    "                'mean_auc_prc':mean_auc_prc,\n",
    "                'std_auc_prc':std_auc_prc,\n",
    "                'mean_prc':mean_precision,\n",
    "                'mean_recall':mean_recall,\n",
    "                'mcc':mccs,\n",
    "                'spec@90':specat90}\n",
    "\n",
    "    def plot_auc(result):\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red',\n",
    "                 label='Random', alpha=.8)\n",
    "\n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        mean_tpr = result['tpr']\n",
    "        mean_auc = result['mean_auc']\n",
    "        std_auc = result['std_auc']\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.4f $\\pm$ %0.4f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_tpr = result['std_tpr']\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "###########################################################################\n",
    "    \n",
    "    #pos\n",
    "    def pos_selection(df_pos,skip_time,min_time):\n",
    "        posl = []\n",
    "        posdf = pd.DataFrame(columns=df_pos.columns)\n",
    "        all_matches = df_pos[df_pos['labelpt'] == df_pos['labelrec']]\n",
    "\n",
    "        for i, p_id in enumerate(all_matches['patientunitstayid'].unique()):\n",
    "            df_p_id = df_pos[df_pos['patientunitstayid'] == p_id]\n",
    "            idx = all_matches[all_matches['patientunitstayid'] == p_id].index[0]\n",
    "            t = df_pos.iloc[idx].itemoffset\n",
    "            if t > (min_time + skip_time):\n",
    "                posl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) & (df_p_id['itemoffset'] > (t-(skip_time+min_time)))])\n",
    "\n",
    "        posdf = pd.concat(posl,axis=0)\n",
    "        return posdf\n",
    "\n",
    "    #neg\n",
    "    def neg_selection(df_neg,skip_time,min_time):\n",
    "        negl = []\n",
    "        negdf = pd.DataFrame(columns=df_neg.columns)\n",
    "\n",
    "        for i, p_id in enumerate(df_neg['patientunitstayid'].unique()):\n",
    "            df_p_id = df_neg[df_neg['patientunitstayid'] == p_id]\n",
    "            t = df_p_id.iloc[-1].itemoffset\n",
    "            if t > (min_time + skip_time):\n",
    "                negl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) &(df_p_id['itemoffset'] > (t-(min_time+skip_time)))])\n",
    "        negdf = pd.concat(negl,axis=0)\n",
    "        return negdf\n",
    "#     eicu_df  = pd.read_csv(\"eicu_df_all_24los_normed.csv\")\n",
    "    mimic_df = pd.read_csv(\"mimic_df_all_24los_normed.csv\")\n",
    "\n",
    " # ADD VASO\n",
    "    mimic_df['vaso_dose'] = np.nan\n",
    "    mimic_df['vaso_dose'] = mimic_df['rate_epinephrine']+mimic_df['rate_norepinephrine'] + mimic_df['rate_phenylephrine']/10 + mimic_df['rate_dopamine']/2\n",
    "    mimic_df.drop(columns=['rate_epinephrine', 'rate_norepinephrine','rate_phenylephrine','rate_dopamine'],inplace=True)\n",
    "\n",
    "#     import pdb;pdb.set_trace()\n",
    "\n",
    "    mimic_pos = mimic_df[mimic_df['CAM']==1]\n",
    "    mimic_neg = mimic_df[mimic_df['CAM']==0]\n",
    "    \n",
    "#     eicu_pos_filtered = pos_selection(eicu_pos,skip_time,min_time)\n",
    "#     eicu_neg_filtered = neg_selection(eicu_neg,skip_time,min_time)\n",
    "#     eicu_df_filtered = pd.concat([eicu_pos_filtered, eicu_neg_filtered],axis=0)\n",
    "\n",
    "    mimic_pos_filtered = pos_selection(mimic_pos,skip_time,min_time)\n",
    "    mimic_neg_filtered = neg_selection(mimic_neg,skip_time,min_time)\n",
    "    mimic_df_filtered = pd.concat([mimic_pos_filtered, mimic_neg_filtered],axis=0)\n",
    "\n",
    "#     trg = eicu_df_filtered.groupby('patientunitstayid')\n",
    "    tsg  = mimic_df_filtered.groupby('patientunitstayid')\n",
    "\n",
    "    idts = []\n",
    "    test_np = []\n",
    "    for idx, frame in tsg:\n",
    "        idts.append(idx)\n",
    "        test_np.append(frame)\n",
    "\n",
    "#     idtr = []\n",
    "#     train_np = []\n",
    "#     for idx, frame in trg:\n",
    "#         idtr.append(idx)\n",
    "#         train_np.append(frame)\n",
    "\n",
    "\n",
    "    columns_ord = ['patientunitstayid', 'itemoffset', 'gender','sofa', 'sofa_wo_gcs', 'age', 'admissionheight',\n",
    "           'admissionweight', 'Heart Rate', 'O2 Saturation', 'glucose',\n",
    "           'Temperature (C)', 'sodium', 'BUN', 'WBC x 1000', 'Hemoglobin',\n",
    "           'Platelets', 'Potassium', 'Chloride', 'Bicarbonate', 'Creatinine',\n",
    "            'vent_flag', 'vaso_dose', 'CAM']\n",
    "\n",
    "    def reader_deli(df_list,verbose=1):\n",
    "        X_noncat = []\n",
    "        X_cat = []\n",
    "        deli = []\n",
    "        nrows = []\n",
    "        ts = []\n",
    "        PID = []\n",
    "        nb_unit_stays = len(df_list)\n",
    "        for i, df in enumerate(df_list):\n",
    "            if verbose:\n",
    "                sys.stdout.write('\\rFeed StayID {0} of {1}...'.format(i+1, nb_unit_stays))\n",
    "            dft = df\n",
    "            dummy = pd.DataFrame(columns=columns_ord)\n",
    "            for c in columns_ord:\n",
    "                dummy[c] = dft[c]        \n",
    "            dft = dummy\n",
    "            narr = np.array(dft)\n",
    "            pid = narr[0,0]\n",
    "            x_cat    = narr[:,2:5]\n",
    "            x_noncat = narr[:, 5:-1]\n",
    "            labeldeli = narr[0, -1]\n",
    "            time = narr[:,1]\n",
    "            X_cat.append(x_cat)\n",
    "            X_noncat.append(x_noncat)\n",
    "            deli.append(labeldeli)\n",
    "            ts.append(time)\n",
    "            nrows.append(narr.shape[0])\n",
    "            PID.append(pid)\n",
    "        PID = np.array(PID)    \n",
    "        X_cat = np.array(X_cat)\n",
    "        X_noncat = np.array(X_noncat)\n",
    "        deli = np.array(deli)\n",
    "        ts= np.array(ts)\n",
    "        return PID,X_cat,X_noncat,ts,nrows,deli\n",
    "    \n",
    "#     PID_tr,X_cat_tr,X_num_tr,ts_tr,nrows_tr,y_tr = reader_deli(train_np)\n",
    "    PID_ts,X_cat_ts,X_num_ts,ts_ts,nrows_ts,y_ts = reader_deli(test_np)\n",
    "    \n",
    "    def pad(arr, max_len= min_time):\n",
    "        tmp = np.zeros((max_len, arr.shape[1]))\n",
    "        tmp[:arr.shape[0], :arr.shape[1]] = arr\n",
    "        return tmp  \n",
    "    def ret_numpy(X):\n",
    "        X_list = []\n",
    "        for n in X:\n",
    "            X_list.append(pad(n))\n",
    "        return np.array(X_list)\n",
    "    \n",
    "#     X_cat_tr = ret_numpy(X_cat_tr)\n",
    "#     X_num_tr = ret_numpy(X_num_tr)\n",
    "\n",
    "    X_cat_ts = ret_numpy(X_cat_ts)\n",
    "    X_num_ts = ret_numpy(X_num_ts)\n",
    "    \n",
    "#     X_train = np.concatenate([X_num_tr,X_cat_tr],axis=-1)\n",
    "#     y_train = y_tr\n",
    "    X_test  = np.concatenate([X_num_ts,X_cat_ts],axis=-1)\n",
    "    y_test  = y_ts\n",
    "\n",
    "    \n",
    "    def data_reader(X,y):\n",
    "        return X,y\n",
    "\n",
    "    \n",
    "    class overdata(Dataset):\n",
    "        def __init__(self, data,label):\n",
    "            self.data  = torch.FloatTensor(data.astype('float'))\n",
    "            self.label = torch.FloatTensor(label.astype('float'))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            label  = self.label[index]\n",
    "            data   = self.data[index]\n",
    "            return data,label\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "\n",
    "    def sampler_fn(data, label):\n",
    "        train_class_sample_count = torch.tensor([(torch.tensor(label) == t).sum() for t in torch.unique(torch.tensor(label), sorted=True)])\n",
    "        train_weight = 1 / train_class_sample_count.float()\n",
    "        train_samples_weight = torch.tensor([train_weight[i] for i in torch.tensor(label).long()])\n",
    "        train_sampler = WeightedRandomSampler(train_samples_weight, len(train_samples_weight))\n",
    "        return train_sampler\n",
    "    \n",
    "    def get_class_weights(data, label):\n",
    "        neg = data[label==0].shape[0]\n",
    "        pos = len(data) - neg\n",
    "        weight_for_0 = (1 / neg)*((pos+neg))/2.0 \n",
    "        weight_for_1 = (1 / pos)*((pos+neg))/2.0\n",
    "        return weight_for_0 ,(weight_for_1)\n",
    "    \n",
    "    if CV:\n",
    "        X_test,y_test= data_reader(X_test,y_test)\n",
    "        \n",
    "    sequence_length = min_time\n",
    "    input_size = 33 \n",
    "        \n",
    "    def run_train(data_loader,params,data,label):\n",
    "    \n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        cw_0,cw_1 = get_class_weights(data,label)\n",
    "        model = BiRNN(input_size, params['hidden_units'], params['num_layers'], num_classes=params['n_classes'],drp = params['dropout']).to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.Tensor([cw_0,cw_1]).to(device))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'],weight_decay=1e-6)\n",
    "\n",
    "        for epoch in range(params['epochs']):\n",
    "\n",
    "            true_labels = []\n",
    "            pred_labels = []\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            for data, labels in data_loader:\n",
    "#                 import pdb;pdb.set_trace()\n",
    "                model.train()\n",
    "                data = torch.FloatTensor(data).to(device)\n",
    "                labels = torch.FloatTensor(labels).to(device)\n",
    "                labels = labels.long()\n",
    "\n",
    "                ##\n",
    "                nonzeros = ~data.cpu().numpy().any(axis=2)\n",
    "                nonzero_index = np.argmax(nonzeros,axis=1)        \n",
    "                nonzero_index = torch.LongTensor(nonzero_index)\n",
    "    #             import pdb;pdb.set_trace()\n",
    "                nonzero_index = torch.where(nonzero_index==0,torch.LongTensor([min_time]),nonzero_index)\n",
    "                seq_lengths, perm_idx = nonzero_index.sort(0, descending=True)\n",
    "\n",
    "                data = data[perm_idx]\n",
    "                labels = labels[perm_idx]\n",
    "                ###\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data[:, :, :18], data[:, : ,18:].long(), seq_lengths)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()        \n",
    "                running_loss += loss.item()\n",
    "            print(\"Epoch: {}/{} \".format(epoch+1, params['epochs']),\"Training Loss: {:.3f} \".format(running_loss/len(data_loader)))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    params = {'lr':0.000075,'num_layers':1,'hidden_units':128,'n_classes':2,\n",
    "              'dropout':0.3,'epochs':50}\n",
    "    \n",
    "    def run_test(data_loader,model):\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "            true_labels, pred_labels = [], []\n",
    "\n",
    "            for data,labels in data_loader:\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.long()\n",
    "                ###\n",
    "                nonzeros = ~data.cpu().numpy().any(axis=2)\n",
    "                data = torch.Tensor(data).to(device)\n",
    "                nonzero_index = np.argmax(nonzeros,axis=1)        \n",
    "                nonzero_index = torch.LongTensor(nonzero_index)\n",
    "                nonzero_index = torch.where(nonzero_index==0,torch.LongTensor([min_time]),nonzero_index)\n",
    "\n",
    "                seq_lengths, perm_idx = nonzero_index.sort(0, descending=True)\n",
    "                data = data[perm_idx]\n",
    "                labels = labels[perm_idx]\n",
    "                ###\n",
    "                outputs = nn.Softmax()(model(data[:, :, :18], data[:, : ,18:].long(), seq_lengths))\n",
    "                pred = outputs.detach().cpu().numpy()\n",
    "                pred = pred[:,1]\n",
    "                true_labels.append(labels)\n",
    "                pred_labels.append(pred)\n",
    "            true_labels = [l.item() for labels in true_labels for l in labels]\n",
    "            pred_labels = [p for labels in pred_labels for p in labels]\n",
    "            pred_labels = np.array(pred_labels)\n",
    "\n",
    "            return true_labels,pred_labels\n",
    "\n",
    "    def model_cv(X,y):\n",
    "        i = 1\n",
    "        cv_scores = {}\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED_VALUE)\n",
    "        for train, test in kfold.split(X, y):\n",
    "            print(f'Fold: {i}')\n",
    "            X_tr = X[train]\n",
    "            X_ts = X[test]\n",
    "            y_tr = y[train]\n",
    "            y_ts = y[test]\n",
    "\n",
    "            data_sampler = sampler_fn(X_tr,y_tr)\n",
    "            train_data = overdata(X_tr,y_tr)\n",
    "            test_data  = overdata(X_ts,y_ts)\n",
    "            train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,shuffle=False,sampler=data_sampler, **kwargs)\n",
    "            test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=False, **kwargs)\n",
    "\n",
    "            model = run_train(train_loader, params, X_tr, y_tr)\n",
    "            savepath = \"mimic_%d_%d_fold_%d_cv_torch.pt\"%(min_time,skip_time,i)\n",
    "            \n",
    "            torch.save(model.state_dict(), savepath)\n",
    "            print(f\"Model:{savepath} saved.\")\n",
    "\n",
    "            y_ts,probs = run_test(test_loader,model)\n",
    "            cv_scores[i] = compute_metrics(X_ts,y_ts,probs)\n",
    "            i += 1        \n",
    "        cum_scores = avg_metrics(cv_scores)\n",
    "#         plot_cum_auc(cum_scores)\n",
    "        return cum_scores\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    models = ['LSTM']\n",
    "    # models = ['LSTM']\n",
    "    results = {}\n",
    "    for m in models:\n",
    "        print(f'**********Model: {m} *********')\n",
    "        results[m] = model_cv(X_test,y_test)\n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "        fpr_tprs = [(mean_fpr, results[m]['tpr']) for m in models]\n",
    "        plot_cum_auc(fpr_tprs, models, savename='{}_cv_mimic_feat.png'.format(TASK),auprc=False)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [12]:\n",
    "    for s in [48]:\n",
    "        get_mimic_feature(m,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data for ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed StayID 773 of 773..."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "min_time = 24\n",
    "skip_time = 96\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "\n",
    "\n",
    "#pos\n",
    "def pos_selection(df_pos,skip_time,min_time):\n",
    "    posl = []\n",
    "    posdf = pd.DataFrame(columns=df_pos.columns)\n",
    "    all_matches = df_pos[df_pos['labelpt'] == df_pos['labelrec']]\n",
    "\n",
    "    for i, p_id in enumerate(all_matches['patientunitstayid'].unique()):\n",
    "        df_p_id = df_pos[df_pos['patientunitstayid'] == p_id]\n",
    "        idx = all_matches[all_matches['patientunitstayid'] == p_id].index[0]\n",
    "        t = df_pos.iloc[idx].itemoffset\n",
    "        if t > (min_time + skip_time):\n",
    "            posl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) & (df_p_id['itemoffset'] > (t-(skip_time+min_time)))])\n",
    "\n",
    "    posdf = pd.concat(posl,axis=0)\n",
    "    return posdf\n",
    "\n",
    "#neg\n",
    "def neg_selection(df_neg,skip_time,min_time):\n",
    "    negl = []\n",
    "    negdf = pd.DataFrame(columns=df_neg.columns)\n",
    "\n",
    "    for i, p_id in enumerate(df_neg['patientunitstayid'].unique()):\n",
    "        df_p_id = df_neg[df_neg['patientunitstayid'] == p_id]\n",
    "        t = df_p_id.iloc[-1].itemoffset\n",
    "        if t > (min_time + skip_time):\n",
    "            negl.append(df_p_id[(df_p_id['itemoffset'] < (t-skip_time)) &(df_p_id['itemoffset'] > (t-(min_time+skip_time)))])\n",
    "    negdf = pd.concat(negl,axis=0)\n",
    "    return negdf\n",
    "mimic_df = pd.read_csv(\"mimic_df_all_24los_normed.csv\")\n",
    "\n",
    "# ADD VASO\n",
    "mimic_df['vaso_dose'] = np.nan\n",
    "mimic_df['vaso_dose'] = mimic_df['rate_epinephrine']+mimic_df['rate_norepinephrine'] + mimic_df['rate_phenylephrine']/10 + mimic_df['rate_dopamine']/2\n",
    "mimic_df.drop(columns=['rate_epinephrine', 'rate_norepinephrine','rate_phenylephrine','rate_dopamine'],inplace=True)\n",
    "\n",
    "mimic_pos = mimic_df[mimic_df['CAM']==1]\n",
    "mimic_neg = mimic_df[mimic_df['CAM']==0]\n",
    "\n",
    "\n",
    "mimic_pos_filtered = pos_selection(mimic_pos,skip_time,min_time)\n",
    "mimic_neg_filtered = neg_selection(mimic_neg,skip_time,min_time)\n",
    "mimic_df_filtered = pd.concat([mimic_pos_filtered, mimic_neg_filtered],axis=0)\n",
    "\n",
    "tsg  = mimic_df_filtered.groupby('patientunitstayid')\n",
    "\n",
    "idts = []\n",
    "test_np = []\n",
    "for idx, frame in tsg:\n",
    "    idts.append(idx)\n",
    "    test_np.append(frame)\n",
    "\n",
    "\n",
    "\n",
    "columns_ord = ['patientunitstayid', 'itemoffset', 'gender','sofa', 'sofa_wo_gcs', 'age', 'admissionheight',\n",
    "       'admissionweight', 'Heart Rate', 'O2 Saturation', 'glucose',\n",
    "       'Temperature (C)', 'sodium', 'BUN', 'WBC x 1000', 'Hemoglobin',\n",
    "       'Platelets', 'Potassium', 'Chloride', 'Bicarbonate', 'Creatinine',\n",
    "        'vent_flag', 'vaso_dose', 'CAM']\n",
    "\n",
    "def reader_deli(df_list,verbose=1):\n",
    "    X_noncat = []\n",
    "    X_cat = []\n",
    "    deli = []\n",
    "    nrows = []\n",
    "    ts = []\n",
    "    PID = []\n",
    "    nb_unit_stays = len(df_list)\n",
    "    for i, df in enumerate(df_list):\n",
    "        if verbose:\n",
    "            sys.stdout.write('\\rFeed StayID {0} of {1}...'.format(i+1, nb_unit_stays))\n",
    "        dft = df\n",
    "        dummy = pd.DataFrame(columns=columns_ord)\n",
    "        for c in columns_ord:\n",
    "            dummy[c] = dft[c]        \n",
    "        dft = dummy\n",
    "        narr = np.array(dft)\n",
    "        pid = narr[0,0]\n",
    "        x_cat    = narr[:,2:5]\n",
    "        x_noncat = narr[:, 5:-1]\n",
    "        labeldeli = narr[0, -1]\n",
    "        time = narr[:,1]\n",
    "        X_cat.append(x_cat)\n",
    "        X_noncat.append(x_noncat)\n",
    "        deli.append(labeldeli)\n",
    "        ts.append(time)\n",
    "        nrows.append(narr.shape[0])\n",
    "        PID.append(pid)\n",
    "    PID = np.array(PID)    \n",
    "    X_cat = np.array(X_cat)\n",
    "    X_noncat = np.array(X_noncat)\n",
    "    deli = np.array(deli)\n",
    "    ts= np.array(ts)\n",
    "    return PID,X_cat,X_noncat,ts,nrows,deli\n",
    "\n",
    "PID_ts,X_cat_ts,X_num_ts,ts_ts,nrows_ts,y_ts = reader_deli(test_np)\n",
    "\n",
    "def pad(arr, max_len= min_time):\n",
    "    tmp = np.zeros((max_len, arr.shape[1]))\n",
    "    tmp[:arr.shape[0], :arr.shape[1]] = arr\n",
    "    return tmp  \n",
    "def ret_numpy(X):\n",
    "    X_list = []\n",
    "    for n in X:\n",
    "        X_list.append(pad(n))\n",
    "    return np.array(X_list)\n",
    "\n",
    "\n",
    "X_cat_ts = ret_numpy(X_cat_ts)\n",
    "X_num_ts = ret_numpy(X_num_ts)\n",
    "\n",
    "X_test  = np.concatenate([X_num_ts,X_cat_ts],axis=-1)\n",
    "y_test  = y_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap import summary_plot\n",
    "\n",
    "def get_feature_ranking_for_shap(model,test_loader,base_loader):\n",
    "\n",
    "    interpretable_embedding = configure_interpretable_embedding_layer(model, \"embeds\")\n",
    "\n",
    "    total_attr_cat, total_attr_num, X_ts_mask, data_ = [], [], [], []\n",
    "    test_labels = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    ig = IntegratedGradients(model)\n",
    "    svs = ShapleyValueSampling(model)\n",
    "    gb = GuidedBackprop(model)\n",
    "    \n",
    "    for (data,labels),(data_base,labels_base) in zip(test_loader,base_loader):\n",
    "        nonzeros = ~data.cpu().numpy().any(axis=2)\n",
    "        data = torch.Tensor(data).to(device)\n",
    "        data_base = torch.Tensor(data_base).to(device)\n",
    "\n",
    "        nonzero_index = np.argmax(nonzeros,axis=1)        \n",
    "        nonzero_index = torch.LongTensor(nonzero_index)\n",
    "        nonzero_index = torch.where(nonzero_index==0,torch.LongTensor([min_time]),nonzero_index)\n",
    "        seq_lengths, perm_idx = nonzero_index.sort(0, descending=True)\n",
    "\n",
    "        data = data[perm_idx]\n",
    "        labels = labels[perm_idx]\n",
    "\n",
    "        data_base = data_base[perm_idx]\n",
    "        labels_base = labels_base[perm_idx]\n",
    "      \n",
    "        X_ts_bool = np.all(data.cpu().numpy()==0, axis = 2)\n",
    "        X_ts_mask.append(X_ts_bool) \n",
    "\n",
    "        categorical = interpretable_embedding.indices_to_embeddings(data[:, :, 18:].long())\n",
    "        numerical   = data[:, : ,:18]\n",
    "\n",
    "        cat_base = interpretable_embedding.indices_to_embeddings(data_base[:, :, 18:].long())\n",
    "        num_base   = data_base[:, : ,:18]\n",
    "\n",
    "        (attr_num,attr_cat) = ig.attribute(inputs=(numerical,categorical),baselines=(num_base,cat_base),additional_forward_args= seq_lengths,target=1)\n",
    "        (attr_num,attr_cat) = svs.attribute(inputs=(numerical,categorical),baselines=(num_base,cat_base),additional_forward_args= seq_lengths,target=1)\n",
    "        (attr_num,attr_cat) = gb.attribute(inputs=(numerical,categorical), additional_forward_args= seq_lengths,target=1)\n",
    "        total_attr_cat.append(attr_cat.detach().cpu().numpy())\n",
    "        total_attr_num.append(attr_num.detach().cpu().numpy())\n",
    "        data_.append(data.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
    "\n",
    "    total_attr_cat = np.vstack(total_attr_cat)\n",
    "    total_attr_cat = np.mean(total_attr_cat,axis=-1)\n",
    "\n",
    "    total_attr_num = np.vstack(total_attr_num)\n",
    "\n",
    "    X_ts_mask = np.vstack(X_ts_mask)\n",
    "\n",
    "    total_attr_1 = np.concatenate([total_attr_num,total_attr_cat],axis=-1)\n",
    "    total_attr_1[X_ts_mask] = np.nan\n",
    "\n",
    "    feature_imp_1 = np.nanmean(total_attr_1, axis=1) #p\n",
    "    \n",
    "    data_for_shap = np.vstack(data_)\n",
    "    \n",
    "    return feature_imp_1,data_for_shap\n",
    "\n",
    "\n",
    "\n",
    "def get_ranking_cv_for_shap(X,y):\n",
    "    i = 1\n",
    "    ranking = []\n",
    "    data_ = []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED_VALUE)\n",
    "    for train, test in kfold.split(X, y):\n",
    "        print(f'Fold: {i}')\n",
    "        X_tr = X[train]\n",
    "        X_ts = X[test]\n",
    "        y_tr = y[train]\n",
    "        y_ts = y[test]\n",
    "\n",
    "        X_base,y_base = get_baseline(X_ts,y_ts)\n",
    "        ########################################################################\n",
    "        params = {'lr':0.000075,'num_layers':1,'hidden_units':128,'n_classes':2,\n",
    "              'dropout':0.3,'epochs':50,'input_size':33}\n",
    "\n",
    "#         LOADPATH = \"mimic_%s_%d_cv_torch.pt\"%(24,96)\n",
    "        LOADPATH = \"mimic_%d_%d_fold_%d_cv_torch.pt\"%(min_time,skip_time,i)\n",
    "\n",
    "        BATCH_SIZE = 128\n",
    "        ########################################################################\n",
    "        print(LOADPATH)\n",
    "        \n",
    "        base_data    = overdata(X_base,y_base)\n",
    "        test_data    = overdata(X_ts,y_ts)\n",
    "        \n",
    "        \n",
    "        base_loader  = DataLoader(base_data, batch_size=BATCH_SIZE,shuffle=False,**kwargs)\n",
    "        test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE,shuffle=False,**kwargs)\n",
    "\n",
    "        model = BiRNN(params['input_size'], params['hidden_units'], params['num_layers'], num_classes=params['n_classes'],drp = params['dropout']).to(device)\n",
    "        model.load_state_dict(torch.load(LOADPATH))\n",
    "        i += 1  \n",
    "        \n",
    "        feat_rank,data_for_shap = get_feature_ranking_for_shap(model,test_loader,base_loader)\n",
    "\n",
    "    \n",
    "        data_.append(data_for_shap)\n",
    "        ranking.append(feat_rank)\n",
    "    \n",
    "    return ranking, data_\n",
    "\n",
    "ranking_per_fold_mimic,data_per_fold_mimic = get_ranking_cv_for_shap(X_test,y_test)\n",
    "\n",
    "\n",
    "def visualize_importances_dot(figname,feature_names, importances, plot=True, axis_title=\"Features\"):\n",
    "    summary_plot(importances,features=np.nanmean(np.vstack(data_per_fold_mimic),axis=1),max_display=21 ,feature_names=feature_name, show=False,plot_type='dot', sort=True, use_log_scale=False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Integrated Gradient')\n",
    "    plt.xlabel('SHAP Values')\n",
    "    plt.xlabel('Gradient Values')\n",
    "    \n",
    "    plt.savefig(figname, dpi=450)\n",
    "    plt.show()\n",
    "\n",
    "savefigname = \"{}min_{}skip_mimic_GB_jama\".format(min_time,skip_time)\n",
    "\n",
    "visualize_importances_dot(savefigname,feature_name, np.vstack(ranking_per_fold_mimic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captumnew",
   "language": "python",
   "name": "captumnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
